{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from future.utils import iteritems\n",
    "from builtins import range, input\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE, LocallyLinearEmbedding as LLE\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Normalization import Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\serena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\serena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num titles: 2373\n",
      "first title: Philosophy of Sex and Love A Reader\n"
     ]
    }
   ],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "titles = [line.rstrip() for line in open('all_book_titles.txt')]\n",
    "\n",
    "# copy tokenizer from sentiment example\n",
    "stopwords = set(w.rstrip() for w in open('stopwords.txt'))\n",
    "# add more stopwords specific to this problem\n",
    "stopwords = stopwords.union({\n",
    "    'introduction', 'edition', 'series', 'application',\n",
    "    'approach', 'card', 'access', 'package', 'plus', 'etext',\n",
    "    'brief', 'vol', 'fundamental', 'guide', 'essential', 'printed',\n",
    "    'third', 'second', 'fourth', })\n",
    "def my_tokenizer(s):\n",
    "    s = s.lower() # downcase\n",
    "    tokens = nltk.tokenize.word_tokenize(s) # split string into words (tokens)\n",
    "    tokens = [t for t in tokens if len(t) > 2] # remove short words, they're probably not useful\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] # put words into base form\n",
    "    tokens = [t for t in tokens if t not in stopwords] # remove stopwords\n",
    "    tokens = [t for t in tokens if not any(c.isdigit() for c in t)] # remove any digits, i.e. \"3rd edition\"\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# create a word-to-index map so that we can create our word-frequency vectors later\n",
    "# let's also save the tokenized versions so we don't have to tokenize again later\n",
    "word_index_map = {}\n",
    "current_index = 0\n",
    "all_tokens = []\n",
    "all_titles = []\n",
    "index_word_map = []\n",
    "print(\"num titles:\", len(titles))\n",
    "print(\"first title:\", titles[0])\n",
    "for title in titles:\n",
    "    try:\n",
    "        title = title.encode('ascii', 'ignore') # this will throw exception if bad characters\n",
    "        title = title.decode('utf-8')\n",
    "        all_titles.append(title)\n",
    "        tokens = my_tokenizer(title)\n",
    "        all_tokens.append(tokens)\n",
    "        for token in tokens:\n",
    "            if token not in word_index_map:\n",
    "                word_index_map[token] = current_index\n",
    "                current_index += 1\n",
    "                index_word_map.append(token)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "\n",
    "# now let's create our input matrices - just indicator variables for this example - works better than proportions\n",
    "def tokens_to_vector(tokens):\n",
    "    x = np.zeros(len(word_index_map))\n",
    "    for t in tokens:\n",
    "        i = word_index_map[t]\n",
    "        x[i] += 1\n",
    "    return x\n",
    "\n",
    "N = len(all_tokens)\n",
    "D = len(word_index_map)\n",
    "X = np.zeros((D, N)) # terms will go along rows, documents along columns\n",
    "i = 0\n",
    "for tokens in all_tokens:\n",
    "    X[:,i] = tokens_to_vector(tokens)\n",
    "    i += 1\n",
    "\n",
    "def d(u, v):\n",
    "    diff = u - v\n",
    "    return diff.dot(diff)\n",
    "\n",
    "def cost(X, R, M):\n",
    "    cost = 0\n",
    "    for k in range(len(M)):\n",
    "        # method 1\n",
    "        # for n in range(len(X)):\n",
    "        #     cost += R[n,k]*d(M[k], X[n])\n",
    "\n",
    "        # method 2\n",
    "        diff = X - M[k]\n",
    "        sq_distances = (diff * diff).sum(axis=1)\n",
    "        cost += (R[:,k] * sq_distances).sum()\n",
    "    return cost\n",
    "\n",
    "def plot_k_means(X, K, index_word_map, max_iter=20, beta=1.0, show_plots=True):\n",
    "    N, D = X.shape\n",
    "    M = np.zeros((K, D))\n",
    "    R = np.zeros((N, K))\n",
    "    exponents = np.empty((N, K))\n",
    "\n",
    "    # initialize M to random\n",
    "    for k in range(K):\n",
    "        M[k] = X[np.random.choice(N)]\n",
    "\n",
    "    costs = np.zeros(max_iter)\n",
    "    for i in range(max_iter):\n",
    "        # step 1: determine assignments / resposibilities\n",
    "        # is this inefficient?\n",
    "        for k in range(K):\n",
    "            for n in range(N):\n",
    "                # R[n,k] = np.exp(-beta*d(M[k], X[n])) / np.sum( np.exp(-beta*d(M[j], X[n])) for j in range(K) )\n",
    "                exponents[n,k] = np.exp(-beta*d(M[k], X[n]))\n",
    "\n",
    "        R = exponents / exponents.sum(axis=1, keepdims=True)\n",
    "\n",
    "        # step 2: recalculate means\n",
    "        for k in range(K):\n",
    "            M[k] = R[:,k].dot(X) / R[:,k].sum()\n",
    "\n",
    "        costs[i] = cost(X, R, M)\n",
    "        if i > 0:\n",
    "            if np.abs(costs[i] - costs[i-1]) < 10e-5:\n",
    "                break\n",
    "\n",
    "    if show_plots:\n",
    "        # plt.plot(costs)\n",
    "        # plt.title(\"Costs\")\n",
    "        # plt.show()\n",
    "\n",
    "        random_colors = np.random.random((K, 3))\n",
    "        colors = R.dot(random_colors)\n",
    "        plt.figure(figsize=(80.0, 80.0))\n",
    "        plt.scatter(X[:,0], X[:,1], s=300, alpha=0.9, c=colors)\n",
    "        annotate1(X, index_word_map)\n",
    "        # plt.show()\n",
    "        plt.savefig(\"test.png\")\n",
    "\n",
    "\n",
    "    # print out the clusters\n",
    "    hard_responsibilities = np.argmax(R, axis=1) # is an N-size array of cluster identities\n",
    "    # let's \"reverse\" the order so it's cluster identity -> word index\n",
    "    cluster2word = {}\n",
    "    for i in range(len(hard_responsibilities)):\n",
    "      word = index_word_map[i]\n",
    "      cluster = hard_responsibilities[i]\n",
    "      if cluster not in cluster2word:\n",
    "        cluster2word[cluster] = []\n",
    "      cluster2word[cluster].append(word)\n",
    "\n",
    "    # print out the words grouped by cluster\n",
    "    for cluster, wordlist in cluster2word.items():\n",
    "      print(\"cluster\", cluster, \"->\", wordlist)\n",
    "\n",
    "    return M, R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_k_means11(X, K, m,max_iter=20, beta=1.0, show_plots=False):\n",
    "    N, D = X.shape\n",
    "    # R = np.zeros((N, K))\n",
    "    exponents = np.empty((N, K))\n",
    "\n",
    "    # initialize M to random\n",
    "    initial_centers = np.random.choice(N, K, replace=False)\n",
    "    M = m\n",
    "\n",
    "    costs = []\n",
    "    k = 0\n",
    "    for i in range(max_iter):\n",
    "        k += 1\n",
    "        # step 1: determine assignments / resposibilities\n",
    "        # is this inefficient?\n",
    "        for k in range(K):\n",
    "            for n in range(N):\n",
    "                exponents[n,k] = np.exp(-beta*d(M[k], X[n]))\n",
    "        R = exponents / (exponents.sum(axis=1, keepdims=True)+0.000000000001)\n",
    "\n",
    "\n",
    "        # step 2: recalculate means\n",
    "        # decent vectorization\n",
    "        # for k in range(K):\n",
    "        #     M[k] = R[:,k].dot(X) / R[:,k].sum()\n",
    "        # oldM = M\n",
    "\n",
    "        # full vectorization\n",
    "        M = R.T.dot(X) / R.sum(axis=0, keepdims=True).T\n",
    "        # print(\"diff M:\", np.abs(M - oldM).sum())\n",
    "\n",
    "        c = cost(X, R, M)\n",
    "        costs.append(c)\n",
    "        if i > 0:\n",
    "            if np.abs(costs[-1] - costs[-2]) < 1e-5:\n",
    "                break\n",
    "\n",
    "        if len(costs) > 1:\n",
    "            if costs[-1] > costs[-2]:\n",
    "                pass\n",
    "                # print(\"cost increased!\")\n",
    "                # print(\"M:\", M)\n",
    "                # print(\"R.min:\", R.min(), \"R.max:\", R.max())\n",
    "\n",
    "    if show_plots:\n",
    "        plt.plot(costs)\n",
    "        plt.title(\"Costs\")\n",
    "        plt.show()\n",
    "\n",
    "        random_colors = np.random.random((K, 3))\n",
    "        colors = R.dot(random_colors)\n",
    "        plt.scatter(X[:,0], X[:,1], c=colors)\n",
    "        plt.show()\n",
    "\n",
    "    #print(\"Final cost\", costs[-1])\n",
    "    return M, R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_k_means2(X, K, max_iter=20, beta=1.0, show_plots=False):\n",
    "    N, D = X.shape\n",
    "    # R = np.zeros((N, K))\n",
    "    exponents = np.empty((N, K))\n",
    "\n",
    "    # initialize M to random\n",
    "    initial_centers = np.random.choice(N, K, replace=False)\n",
    "    M = X[initial_centers]\n",
    "\n",
    "    costs = []\n",
    "    k = 0\n",
    "    for i in range(max_iter):\n",
    "        k += 1\n",
    "        # step 1: determine assignments / resposibilities\n",
    "        # is this inefficient?\n",
    "        for k in range(K):\n",
    "            for n in range(N):\n",
    "                exponents[n,k] = np.exp(-beta*d(M[k], X[n]))\n",
    "        R = exponents / exponents.sum(axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "        # step 2: recalculate means\n",
    "        # decent vectorization\n",
    "        # for k in range(K):\n",
    "        #     M[k] = R[:,k].dot(X) / R[:,k].sum()\n",
    "        # oldM = M\n",
    "\n",
    "        # full vectorization\n",
    "        M = R.T.dot(X) / R.sum(axis=0, keepdims=True).T\n",
    "        # print(\"diff M:\", np.abs(M - oldM).sum())\n",
    "\n",
    "        c = cost(X, R, M)\n",
    "        costs.append(c)\n",
    "        if i > 0:\n",
    "            if np.abs(costs[-1] - costs[-2]) < 1e-5:\n",
    "                break\n",
    "\n",
    "        if len(costs) > 1:\n",
    "            if costs[-1] > costs[-2]:\n",
    "                pass\n",
    "                # print(\"cost increased!\")\n",
    "                # print(\"M:\", M)\n",
    "                # print(\"R.min:\", R.min(), \"R.max:\", R.max())\n",
    "\n",
    "    if show_plots:\n",
    "        plt.plot(costs)\n",
    "        plt.title(\"Costs\")\n",
    "        plt.show()\n",
    "\n",
    "        random_colors = np.random.random((K, 3))\n",
    "        colors = R.dot(random_colors)\n",
    "        plt.scatter(X[:,0], X[:,1], c=colors)\n",
    "        plt.show()\n",
    "    return M, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tri_k_means(X, K1, index_word_map, max_iter=20, beta=1.0, show_plots=True):\n",
    "    \n",
    "    X = Normalization(X)\n",
    "    \n",
    "    K = int(math.sqrt(K1))\n",
    "    M,R = plot_k_means2(X,K,beta=20,show_plots=False)\n",
    "    \n",
    "    ######### Trilevel  Fully divided ###########       \n",
    "    \n",
    "    label = np.argmax(R,axis=1)\n",
    "    gnum = K\n",
    "    Group = []\n",
    "\n",
    "    for i in range(gnum):\n",
    "        index = np.where(label==i)\n",
    "        print(np.asarray(X[index,:]).shape)\n",
    "        x,y,z = np.asarray(X[index,:]).shape\n",
    "        Group.append(np.asarray(X[index,:]).reshape((y,z)))\n",
    "    \n",
    "    #comupute the standard diviation \n",
    "    std = []\n",
    "    \n",
    "    for i in range(len(Group)):\n",
    "        std.append(np.average(np.std(Group[i],axis=1)))\n",
    "\n",
    "\n",
    "    #compute the Kc\n",
    "    KC = []\n",
    "    k = []\n",
    "    for i in range(gnum):\n",
    "        KC.append(Group[i].shape[1]*std[i])\n",
    "\n",
    "        \n",
    "    for i in range(gnum):\n",
    "        k.append (round(KC[i]/(sum(KC))*K1)) \n",
    "        \n",
    "    k[-1] = K1 - sum(k[:-1])\n",
    "    \n",
    "    M = []\n",
    "    R = []\n",
    "    \n",
    "    # subclass\n",
    "    for i in range(gnum):\n",
    "        M1,R1 = plot_k_means2(Group[i],int(k[i]),beta=20,show_plots=False)\n",
    "        M.append(M1)\n",
    "        R.append(R1)\n",
    "    \n",
    "    Mnew = np.ones((1,z))\n",
    "    for i in range(gnum):\n",
    "        Mnew = np.concatenate((Mnew,M[i]),axis=0)\n",
    "    Mnew = Mnew[1:,:]\n",
    "    \n",
    "    MM,RR = plot_k_means11(X,K1,Mnew,beta=20)\n",
    "    \n",
    "    if show_plots:\n",
    "        \n",
    "        random_colors = np.random.random((K1, 3))\n",
    "        colors = RR.dot(random_colors)\n",
    "        plt.figure(figsize=(80.0, 80.0))\n",
    "        plt.scatter(X[:,0], X[:,1], s=300, alpha=0.9, c=colors)\n",
    "        annotate1(X, index_word_map)\n",
    "        # plt.show()\n",
    "        plt.savefig(\"test.png\")\n",
    "\n",
    "    # print out the clusters\n",
    "    \"\"\"\n",
    "      hard_responsibilities = np.argmax(R, axis=1) # is an N-size array of cluster identities\n",
    "        # let's \"reverse\" the order so it's cluster identity -> word index\n",
    "        cluster2word = {}\n",
    "        for i in range(len(hard_responsibilities)):\n",
    "          word = index_word_map[i]\n",
    "          cluster = hard_responsibilities[i]\n",
    "          if cluster not in cluster2word:\n",
    "            cluster2word[cluster] = []\n",
    "          cluster2word[cluster].append(word)\n",
    "\n",
    "        # print out the words grouped by cluster\n",
    "        for cluster, wordlist in cluster2word.items():\n",
    "          print(\"cluster\", cluster, \"->\", wordlist)\n",
    "      \n",
    "   \"\"\"\n",
    "\n",
    "    return MM, RR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 2069\n"
     ]
    }
   ],
   "source": [
    "def annotate1(X, index_word_map, eps=0.1):\n",
    "  N, D = X.shape\n",
    "  placed = np.empty((N, D))\n",
    "  for i in range(N):\n",
    "    x, y = X[i]\n",
    "\n",
    "    # if x, y is too close to something already plotted, move it\n",
    "    close = []\n",
    "\n",
    "    x, y = X[i]\n",
    "    for retry in range(3):\n",
    "      for j in range(i):\n",
    "        diff = np.array([x, y]) - placed[j]\n",
    "\n",
    "        # if something is close, append it to the close list\n",
    "        if diff.dot(diff) < eps:\n",
    "          close.append(placed[j])\n",
    "\n",
    "      if close:\n",
    "        # then the close list is not empty\n",
    "        x += (np.random.randn() + 0.5) * (1 if np.random.rand() < 0.5 else -1)\n",
    "        y += (np.random.randn() + 0.5) * (1 if np.random.rand() < 0.5 else -1)\n",
    "        close = [] # so we can start again with an empty list\n",
    "      else:\n",
    "        # nothing close, let's break\n",
    "        break\n",
    "\n",
    "    placed[i] = (x, y)\n",
    "\n",
    "    plt.annotate(\n",
    "      s=index_word_map[i],\n",
    "      xy=(X[i,0], X[i,1]),\n",
    "      xytext=(x, y),\n",
    "      arrowprops={\n",
    "        'arrowstyle' : '->',\n",
    "        'color' : 'black',\n",
    "      }\n",
    "    )\n",
    "\n",
    "print(\"vocab size:\", current_index)\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "X = transformer.fit_transform(X).toarray()\n",
    "\n",
    "reducer = TSNE()\n",
    "Z = reducer.fit_transform(X)\n",
    "#plot_tri_k_means(Z[:,:2], current_index//10, index_word_map, show_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 185, 2)\n",
      "(1, 95, 2)\n",
      "(1, 217, 2)\n",
      "(1, 189, 2)\n",
      "(1, 180, 2)\n",
      "(1, 73, 2)\n",
      "(1, 164, 2)\n",
      "(1, 213, 2)\n",
      "(1, 144, 2)\n",
      "(1, 128, 2)\n",
      "(1, 68, 2)\n",
      "(1, 168, 2)\n",
      "(1, 173, 2)\n",
      "(1, 72, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.36288225, 0.43889597],\n",
       "        [0.3628559 , 0.43889455],\n",
       "        [0.36288142, 0.43889587],\n",
       "        [0.36289044, 0.43889643],\n",
       "        [0.36284516, 0.43889422],\n",
       "        [0.36284507, 0.43889431],\n",
       "        [0.3628862 , 0.43889615],\n",
       "        [0.36287678, 0.4388957 ],\n",
       "        [0.36287973, 0.43889591],\n",
       "        [0.6415657 , 0.5173216 ],\n",
       "        [0.64156348, 0.51731893],\n",
       "        [0.64156315, 0.51731853],\n",
       "        [0.64155775, 0.51731208],\n",
       "        [0.64156054, 0.51731545],\n",
       "        [0.64156243, 0.51731768],\n",
       "        [0.6415574 , 0.51731167],\n",
       "        [0.64155956, 0.51731425],\n",
       "        [0.64155841, 0.51731287],\n",
       "        [0.64156163, 0.51731673],\n",
       "        [0.51222342, 0.73464577],\n",
       "        [0.51222191, 0.73464435],\n",
       "        [0.5122115 , 0.73464766],\n",
       "        [0.51222887, 0.73464368],\n",
       "        [0.51223011, 0.73464491],\n",
       "        [0.51224963, 0.73464195],\n",
       "        [0.5122385 , 0.73464234],\n",
       "        [0.51224364, 0.73464204],\n",
       "        [0.51222128, 0.73464521],\n",
       "        [0.51221788, 0.73464533],\n",
       "        [0.51222857, 0.73464438],\n",
       "        [0.51222285, 0.73464587],\n",
       "        [0.51223993, 0.73464317],\n",
       "        [0.51222072, 0.73464691],\n",
       "        [0.51224141, 0.7346423 ],\n",
       "        [0.51222465, 0.7346444 ],\n",
       "        [0.51221744, 0.73464744],\n",
       "        [0.51221744, 0.73464744],\n",
       "        [0.51224258, 0.73464184],\n",
       "        [0.51221934, 0.73464561],\n",
       "        [0.51221557, 0.73464613],\n",
       "        [0.51222684, 0.73464506],\n",
       "        [0.51224848, 0.73464188],\n",
       "        [0.47152439, 0.3170602 ],\n",
       "        [0.47150288, 0.31709358],\n",
       "        [0.47152909, 0.31705365],\n",
       "        [0.47150499, 0.31708994],\n",
       "        [0.47152975, 0.31705226],\n",
       "        [0.47152334, 0.31706139],\n",
       "        [0.47151488, 0.31707513],\n",
       "        [0.47151067, 0.31708178],\n",
       "        [0.47151269, 0.31707881],\n",
       "        [0.47152961, 0.31705245],\n",
       "        [0.4715102 , 0.31708258],\n",
       "        [0.4715041 , 0.31709146],\n",
       "        [0.56154695, 0.30461859],\n",
       "        [0.56154865, 0.30462559],\n",
       "        [0.56154317, 0.30460305],\n",
       "        [0.5615445 , 0.30460842],\n",
       "        [0.56154317, 0.30460305],\n",
       "        [0.56154875, 0.30462604],\n",
       "        [0.5615429 , 0.30460179],\n",
       "        [0.56154117, 0.30459468],\n",
       "        [0.56155011, 0.3046316 ],\n",
       "        [0.56154157, 0.30459644],\n",
       "        [0.56154521, 0.30461135],\n",
       "        [0.56155248, 0.30464142],\n",
       "        [0.5615409 , 0.30459373],\n",
       "        [0.56155131, 0.30463657],\n",
       "        [0.56154177, 0.30459726],\n",
       "        [0.56154837, 0.30462441],\n",
       "        [0.56154888, 0.30462649],\n",
       "        [0.56154391, 0.30460598],\n",
       "        [0.56153979, 0.3045891 ],\n",
       "        [0.56154474, 0.30460945],\n",
       "        [0.56155041, 0.30463289],\n",
       "        [0.56154374, 0.30460528],\n",
       "        [0.56154858, 0.30462532],\n",
       "        [0.56155039, 0.3046328 ],\n",
       "        [0.56154474, 0.30460942],\n",
       "        [0.56154904, 0.30462717],\n",
       "        [0.5615512 , 0.30463615],\n",
       "        [0.50749805, 0.38536041],\n",
       "        [0.5074931 , 0.38536164],\n",
       "        [0.50750221, 0.38536506],\n",
       "        [0.50749121, 0.38536333],\n",
       "        [0.50749659, 0.38536548],\n",
       "        [0.50749005, 0.38536451],\n",
       "        [0.5074949 , 0.38536527],\n",
       "        [0.50750021, 0.38536471],\n",
       "        [0.5074935 , 0.3853612 ],\n",
       "        [0.62675716, 0.64824916],\n",
       "        [0.62674854, 0.64823827],\n",
       "        [0.62675646, 0.64824776],\n",
       "        [0.62674804, 0.64823753],\n",
       "        [0.62675143, 0.64824202],\n",
       "        [0.71452136, 0.52818168],\n",
       "        [0.71451186, 0.52820287],\n",
       "        [0.71451169, 0.52820344],\n",
       "        [0.71452713, 0.52816786],\n",
       "        [0.71450987, 0.52821223],\n",
       "        [0.71451856, 0.52819059],\n",
       "        [0.71451937, 0.52818653],\n",
       "        [0.71451556, 0.52819391],\n",
       "        [0.71451524, 0.52819804],\n",
       "        [0.71451548, 0.52819411],\n",
       "        [0.71453085, 0.52816063],\n",
       "        [0.71453756, 0.52814402],\n",
       "        [0.71451431, 0.52819868],\n",
       "        [0.71452183, 0.52817934],\n",
       "        [0.71450307, 0.52822791],\n",
       "        [0.71451296, 0.52820086],\n",
       "        [0.71452059, 0.52818223],\n",
       "        [0.7145114 , 0.52820401],\n",
       "        [0.71451186, 0.52820287],\n",
       "        [0.71452155, 0.52818082],\n",
       "        [0.6891305 , 0.43424836],\n",
       "        [0.68911142, 0.43426532],\n",
       "        [0.68911881, 0.43425875],\n",
       "        [0.68910432, 0.43427163],\n",
       "        [0.68910215, 0.43427357],\n",
       "        [0.68909857, 0.43427674],\n",
       "        [0.68910803, 0.43426834],\n",
       "        [0.68913101, 0.4342479 ],\n",
       "        [0.68910503, 0.434271  ],\n",
       "        [0.68913101, 0.4342479 ],\n",
       "        [0.68910925, 0.43426725],\n",
       "        [0.68910112, 0.43427448],\n",
       "        [0.68910259, 0.43427318],\n",
       "        [0.68910179, 0.43427388],\n",
       "        [0.68910574, 0.43427038],\n",
       "        [0.68911564, 0.43426158],\n",
       "        [0.68910503, 0.434271  ],\n",
       "        [0.68909861, 0.43427671],\n",
       "        [0.68913893, 0.43424085],\n",
       "        [0.68910904, 0.43426744],\n",
       "        [0.68910282, 0.43427297],\n",
       "        [0.68912395, 0.43425418],\n",
       "        [0.68910766, 0.43426867],\n",
       "        [0.68911529, 0.43426189],\n",
       "        [0.68910583, 0.4342703 ],\n",
       "        [0.68911589, 0.43426135],\n",
       "        [0.68909976, 0.43427569],\n",
       "        [0.29823381, 0.53253039],\n",
       "        [0.29822037, 0.53253503],\n",
       "        [0.29824205, 0.53252754],\n",
       "        [0.29823381, 0.53253039],\n",
       "        [0.2982442 , 0.5325268 ],\n",
       "        [0.29824787, 0.53252553],\n",
       "        [0.29824878, 0.53252522],\n",
       "        [0.29823592, 0.53252966],\n",
       "        [0.29824075, 0.53252799],\n",
       "        [0.29822239, 0.53253433],\n",
       "        [0.29822676, 0.53253282],\n",
       "        [0.2982159 , 0.53253658],\n",
       "        [0.29822762, 0.53253253],\n",
       "        [0.29821399, 0.53253723],\n",
       "        [0.29823006, 0.53253168],\n",
       "        [0.2982465 , 0.53252601],\n",
       "        [0.29821337, 0.53253745],\n",
       "        [0.29822972, 0.5325318 ],\n",
       "        [0.29822912, 0.53253201],\n",
       "        [0.29821242, 0.53253778],\n",
       "        [0.29823951, 0.53252842],\n",
       "        [0.29822545, 0.53253328],\n",
       "        [0.29823579, 0.53252971],\n",
       "        [0.29823148, 0.53253119],\n",
       "        [0.29821565, 0.53253666],\n",
       "        [0.61222446, 0.57128355],\n",
       "        [0.61222157, 0.5712833 ],\n",
       "        [0.61221849, 0.57128169],\n",
       "        [0.50623677, 0.4824312 ],\n",
       "        [0.50623527, 0.48243482],\n",
       "        [0.50623565, 0.48243058],\n",
       "        [0.50623835, 0.48243039],\n",
       "        [0.38933818, 0.64205906],\n",
       "        [0.38931988, 0.64209299],\n",
       "        [0.38933931, 0.64205698],\n",
       "        [0.38932276, 0.64208764],\n",
       "        [0.38932917, 0.64207576],\n",
       "        [0.3893107 , 0.64211006],\n",
       "        [0.38934058, 0.64205463],\n",
       "        [0.38933852, 0.64205844],\n",
       "        [0.38934278, 0.64205055],\n",
       "        [0.38934006, 0.64205559],\n",
       "        [0.38932531, 0.64208291],\n",
       "        [0.38933246, 0.64206965],\n",
       "        [0.38933007, 0.64207408],\n",
       "        [0.38932829, 0.64207738],\n",
       "        [0.38930981, 0.6421117 ],\n",
       "        [0.38934138, 0.64205315],\n",
       "        [0.38932683, 0.6420801 ],\n",
       "        [0.38933732, 0.64206066],\n",
       "        [0.38934343, 0.64204937],\n",
       "        [0.38934112, 0.64205363],\n",
       "        [0.38933759, 0.64206017],\n",
       "        [0.38933127, 0.64207186],\n",
       "        [0.38933923, 0.64205713],\n",
       "        [0.38931065, 0.64211015],\n",
       "        [0.38933566, 0.64206373],\n",
       "        [0.5509273 , 0.646735  ],\n",
       "        [0.55092161, 0.64672675],\n",
       "        [0.55092054, 0.6467254 ],\n",
       "        [0.5509235 , 0.64673045],\n",
       "        [0.55092812, 0.64673837],\n",
       "        [0.55093207, 0.64673919],\n",
       "        [0.55092016, 0.64672529]]),\n",
       " array([[0.00340151, 0.00340069, 0.00340149, ..., 0.00715732, 0.00715732,\n",
       "         0.00715771],\n",
       "        [0.00117259, 0.00117228, 0.00117258, ..., 0.00010568, 0.00010568,\n",
       "         0.00010571],\n",
       "        [0.00094174, 0.00094145, 0.00094173, ..., 0.00014008, 0.00014008,\n",
       "         0.00014011],\n",
       "        ...,\n",
       "        [0.00816332, 0.00816344, 0.00816332, ..., 0.00507049, 0.00507031,\n",
       "         0.00507098],\n",
       "        [0.00844053, 0.00844066, 0.00844053, ..., 0.00495389, 0.00495371,\n",
       "         0.00495439],\n",
       "        [0.0083411 , 0.00834126, 0.0083411 , ..., 0.00492744, 0.00492726,\n",
       "         0.00492793]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_tri_k_means(Z[:,:2], current_index//10, index_word_map, show_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
